---
title: "MATH 216 Homework 3"
author: "CARTER \"DangeR\" MEREN$TEIN"
output: html_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))

suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(Quandl))
suppressPackageStartupMessages(library(boot))
```


## Admistrative:

Please indicate

* Who you collaborated with: No one
* Roughly how much time you spent on this HW: 8 hours
* What gave you the most trouble: Logistic regression. I think I didn't pay enough attention to all the math
* Any comments you have: I just learned that R version nicknames are all Peanuts references!


## Data

* You must first copy the file `profiles.csv` from `HW-2` to the `data` folder
in the `HW-3` directory
* We also consider all 222,540 songs played in the Reed College pool hall
jukebox from Nov 30, 2003 to Jan 22, 2009 (included in `HW-3` folder). 

```{r, echo=FALSE, cache=TRUE}
# DO NOT EDIT THIS SECTION!
profiles <- read.csv("data/profiles.csv", header=TRUE) %>% 
  tbl_df()
jukebox <- read.csv("data/jukebox.csv", header=TRUE) %>% 
  tbl_df()

find.query <- function(char.vector, query){
  which.has.query <- grep(query, char.vector, ignore.case = TRUE)
  length(which.has.query) != 0
}
profile.has.query <- function(data.frame, query){
  query <- tolower(query)
  has.query <- apply(data.frame, 1, find.query, query=query)
  return(has.query)
}

profiles <- mutate(profiles, is_female = ifelse(sex=="f", 1, 0))

```





## Question 1:

For this question we will be picking up from where we left off in HW-2,
specifically the OkCupid dataset.


### a)

Using your exploratory data analysis from HW-2, fit a logistic regression to
predict individual's gender and interpret your results.

```{r, echo=FALSE, fig.width=12, fig.height=6}
profiles <- filter(profiles, !is.na(height))

essays <- select(profiles, contains("essay"))

profiles$has_heels <- profile.has.query(data.frame = essays, query = "heels")

p1 <- ggplot(data=profiles, aes(x=has_heels, y=is_female)) + 
  xlab("has heels") + ylab("is female?")
p1 +
  geom_jitter(width=0.5, height=0.5)

profiles$has_baking <- profile.has.query(data.frame = essays, query = "baking")

p1 <- ggplot(data=profiles, aes(x=has_baking, y=is_female)) + 
  xlab("has baking") + ylab("is female?")
p1 +
  geom_jitter(width=0.5, height=0.5)

model1 <- glm(is_female ~ height + (has_heels + has_baking), data=profiles, family=binomial())

summary(model1)

b1 <- coefficients(model1)
#b1
b1[2]
inv.logit(b1[3])
inv.logit(b1[4])

p2 <- ggplot(data=profiles, aes(x=height, y=is_female)) +
  xlab("height") + ylab("is female") +
  xlim(c(50, 80)) + 
  geom_jitter(height=0.2) 
p2



#p1 + 
#  geom_hline(yintercept=1/(1+exp(-(b1[1] + 0*b1[2]))), col="red", size=2)+
#  geom_hline(yintercept=1/(1+exp(-(b1[1] + 1*b1[2]))), col="blue", size=2)

```
> The three factors above go into the model, which gives the fitted probabilities graphed below. The coefficients need to be inverse logit transformed because it's a logistic regression. So having "heels" or "baking" increases your chance of being female 80%! But I don't think that the hieght gets transformed like that too, because it's continuous? The inverse logit of the height coefficient seems way to high to make sense, but the actual coefficient makes sense as each inch decreasing the probability of being female by 0.64%. 

### b)

Plot a histogram of the fitted probabilities $\widehat{p}_i$ for all users $i=1,
\ldots, n=59946$ in your dataset.

```{r, echo=FALSE, fig.width=12, fig.height=6}
hist(fitted(model1))

#fitted(model1)

```
> More observations fall on the ends, and there isn't much in the .4-.6 range. This is good. We don't want things to be in the middle because this means equally likely to be male or female.


### c)

Use a *decision threshold* of $p^*=0.5$ to make an explicit prediction for each
user $i$'s sex and save this in a variable `predicted_sex`. In other words, for user $i$

* If $\widehat{p}_i > p^*$, set `predicted_sex = 1` i.e. they are female
* If $\widehat{p}_i < p^*$, set `predicted_sex = 0` i.e. they are male

Display a 2 x 2 contigency table of `sex` and `predicted_sex` i.e. compare the 
predicted sex to the actual sex of all users. The sum of all the elements in
your table should be $n=59946$. Comment on how well our predictions fared.

```{r, echo=FALSE, fig.width=12, fig.height=6}
predict = c( sum(model1$fitted.values >0.5), sum(model1$fitted.values <0.5))

actual = c(sum(profiles$is_female==1), sum(profiles$is_female==0))

contig_table <- rbind(predict, actual)
contig_table

```
> It seems like it gets equal amounts almost. I couldn't merge the fitted vales back into the orgional profiles though, so I don't have a real sense of false positives or false negatives.

### d)

Say we wanted to have a **false positive rate** of about 20%, i.e. of the people
we predicted to be female, we want to be wrong no more than 20% of the time. What
decision threshold $p^*$ should we use?

```{r, echo=FALSE, fig.width=12, fig.height=6}

```





## Question 2:

Using the jukebox data, plot a time series of the number of songs played each
week over the entire time period. i.e.

* On the x-axis present actual dates (not something like Week 93, which doesn't 
mean anything to most people).
* On the y-axis present the total number of songs.

What seasonal (i.e. cyclical) patterns do you observe?

```{r, echo=FALSE, fig.width=12, fig.height=6}
attach(jukebox)

jukebox <- mutate(jukebox, parsed_date_time = parse_date_time(date_time, "%b %d %H%M%S %y"))

jukebox <- separate(jukebox, col=parsed_date_time, into=c("date", "time"), sep = " ")

jukebox$date <- as.Date(jukebox$date, "%Y-%m-%d")

#jukebox$Month <- as.Date(cut(jukebox$date, breaks = "month"))
jukebox$Week <- as.Date(cut(jukebox$date, breaks = "week"))

q2 <- jukebox %>% group_by(Week) %>% summarize(n = n())
#q2

p2 <- ggplot(q2, aes(x = Week, y = n)) + geom_line(stat="identity")
p2
```

> as one would expect, the summers and winter break there's way fewer plays.



## Question 3:

Using the jukebox data, what are the top 10 artists played during the "graveyard
shift" during the academic year? Define

* the "graveyard shift" as midnight to 8am
* the academic year as September through May (inclusive)

```{r, echo=FALSE, fig.width=12, fig.height=6}

jukebox <- separate(jukebox, col=time, into=c("Hour", "Minute", "Second"), sep = ":") %>% 
  separate(col=date_time, into=c("day", "Month_name", "junk"), sep = " ")

jukebox <- mutate(jukebox, Hour = as.numeric(Hour))

q3 <- jukebox %>% filter(Hour < 8 ) %>% filter(Month_name %in% c("Sep" , "Oct" , "Nov" , "Dec" , "Jan" , "Feb" , "Mar" , "Apr" , "May"))

q3 <- group_by(q3, artist) %>% summarize(count = n())


q3 <- top_n(q3, 10, count)

q3$artist <- factor(q3$artist, levels = q3$artist[order(-q3$count)])
#I finally figured out how to order bar charts! ("figured out" = looked at Ali's HW1...)

p3 <- ggplot(q3, aes(x = artist, y = count)) + geom_bar(stat="identity")
p3 <- p3 + theme(axis.text.x = element_text(angle = 45, hjust = 1))
p3

```

> It doesn't change much when you don't limit it to the school year. This is probably because the greater number of plays durring the school year drive what makes the top anyway.



## Question 4:

We want to compare the volatility of 

* bitcoin prices
* gold prices

Let our measure of volatility be the relative change from day-to-day in price. 
Let the reference currency be US dollars. Analyze these results and provide
insight to a foreign currency exchanger.

```{r, echo=FALSE, fig.width=12, fig.height=6}

gold <- Quandl("BUNDESBANK/BBK01_WT5511") %>% tbl_df() %>% 
  mutate(type = "gold") %>% 
  mutate( diff = abs(Value - lag(Value))/lag(Value)) %>% 
  mutate(date_copy = Date)

gold <- separate(gold, col= date_copy, into=c("year", "Month", "Day"), sep = "-" ) %>% 
  filter(as.numeric(year) >= 2010)

bitcoin <- Quandl("BAVERAGE/USD") %>% tbl_df() %>% 
  rename(Value = `24h Average`) %>% 
  mutate( type = "bitcoin") %>% 
  mutate(bitcoin, diff = abs(Value - lag(Value))/lag(Value))


both <- rbind( select(gold, 1:4), select(bitcoin, 1:2,7:8))

ggplot(both, aes(x = Date, y = Value, color = type)) + geom_point()
```

> Just looking at the values, they seem similar, but we can look at the day to day difference directly below.

```{r, echo=FALSE, fig.width=12, fig.height=6}

ggplot(both, aes(x = Date, y = diff, color = type)) + geom_point()
```

> In total bitcoin appears less volitile, but this is really just because of an initial period of calm, when the currency was laregely undiscovered. Since the initial boom and bust, the volitility of bitcoin has been pretty on par with that of gold. I'm a bio major; I haven't the faintest idea what this means, but looking at this graph I'd rather have a pile of gold than a stash of bitcoin.

```{r, echo=FALSE, fig.width=12, fig.height=6}
gold <- Quandl("BUNDESBANK/BBK01_WT5511") %>% tbl_df() %>% 
    mutate(type = "gold") %>% 
    mutate(diff = abs(Value - lag(Value))) %>% 
    mutate(date_copy = Date)
gold <- separate(gold, col= date_copy, into=c("year", "Month", "Day"), sep = "-" )

both <- rbind( select(gold, 1:4), select(bitcoin, 1:2,7:8))

ggplot(both, aes(x = Date, y = diff, color = type)) + geom_point()


```
> In the long run though, gold seems less volitile. It's hard to tell how bitcoin would have done if it were around in the 90s and early 2000s when gold was doing well.

## Question 5:

Using the data loaded from Quandl below, plot a time series using `geom_line()`
comparing cheese and milk production in the US from 1930 to today. Comment on this.

* Cheese [page](https://www.quandl.com/data/USDANASS/NASS_CHEESEPRODUCTIONMEASUREDINLB-Cheese-Production-Measured-In-Lb)
* Milk [page](https://www.quandl.com/data/USDANASS/NASS_MILKPRODUCTIONMEASUREDINLB-Milk-Production-Measured-In-Lb)

```{r, echo=FALSE, fig.width=12, fig.height=6}
cheese <- Quandl("USDANASS/NASS_CHEESEPRODUCTIONMEASUREDINLB") %>% 
  tbl_df()
milk <-  Quandl("USDANASS/NASS_MILKPRODUCTIONMEASUREDINLB") %>% 
  tbl_df()

cheese <- mutate(cheese, type = "cheese")
milk <- mutate(milk, type = "milk")

both <- rbind(cheese, milk)

ggplot(both, aes(x = Date, y = Value, color = type)) + geom_line()
```
> Cheese doesn't really seem to depend on the price of milk. This is surprising, obviously, because milk makes cheese. That said, it takes a few years to make cheese, and so cheese companies always have a lot of cheese that's aging in storage. This probably helps reduce the effect of milk prices, since a spike can be distributed across a few years. 

```{r, echo=FALSE, fig.width=12, fig.height=6}

milk <-  Quandl("USDANASS/NASS_MILKPRODUCTIONMEASUREDINLB") %>% 
  tbl_df()

milk <- mutate(milk, Value = Value/50) # I couldn't figure out what you were getting at with a better transform. swrt and log both don't look good,  nor did other roots.

cheese <- mutate(cheese, type = "cheese")
milk <- mutate(milk, type = "milk")

both <- rbind(cheese, milk)

ggplot(both, aes(x = Date, y = Value, color = type)) + geom_line()
```

> The lack of movement in the cheese prices is partially just a funciton of the scale of each. Cheese is a lot cheaper than milk (at least, in the way this is reported. Presumably that's not true pound-for-point or no one would make cheese). When they're on the same scale (I just divided milk prices by 50), you can see that they do look somewhat similar, especially from 1940 to the mid 60s. It's harder to tell trends when they both start increasing, but some are still noticable, like the spike in year 2000, or in the mid 80s.