---
title: "MATH 216 Homework 3"
author: "CARTER \"sorry this isn't finished\" MEREN$TEIN"
output: html_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))

suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(Quandl))
```


## Admistrative:

Please indicate

* Who you collaborated with: No one
* Roughly how much time you spent on this HW: 4 hours
* What gave you the most trouble: I missed the lecture(s) on Quandl and lubridate (I was home for a funeral) so I have to learn that better still.
* Any comments you have: Sorry this isn't finished :( There's a lot going wrong with my thesis so I've just had to be in lab the last two weeks.


## Data

* You must first copy the file `profiles.csv` from `HW-2` to the `data` folder
in the `HW-3` directory
* We also consider all 222,540 songs played in the Reed College pool hall
jukebox from Nov 30, 2003 to Jan 22, 2009 (included in `HW-3` folder). 

```{r, echo=FALSE, cache=TRUE}
# DO NOT EDIT THIS SECTION!
profiles <- read.csv("data/profiles.csv", header=TRUE) %>% 
  tbl_df()
jukebox <- read.csv("data/jukebox.csv", header=TRUE) %>% 
  tbl_df()

find.query <- function(char.vector, query){
  which.has.query <- grep(query, char.vector, ignore.case = TRUE)
  length(which.has.query) != 0
}
profile.has.query <- function(data.frame, query){
  query <- tolower(query)
  has.query <- apply(data.frame, 1, find.query, query=query)
  return(has.query)
}

profiles <- mutate(profiles, is_female = ifelse(sex=="f", 1, 0))

```





## Question 1:

For this question we will be picking up from where we left off in HW-2,
specifically the OkCupid dataset.


### a)

Using your exploratory data analysis from HW-2, fit a logistic regression to
predict individual's gender and interpret your results.

```{r, echo=FALSE, fig.width=12, fig.height=6}
essays <- select(profiles, contains("essay"))

profiles$has_heels <- profile.has.query(data.frame = essays, query = "heels")

p1 <- ggplot(data=profiles, aes(x=has_heels, y=is_female)) + 
  xlab("has heels") + ylab("is female?")
p1 +
  geom_jitter(width=0.5, height=0.5)

profiles$has_baking <- profile.has.query(data.frame = essays, query = "baking")

p1 <- ggplot(data=profiles, aes(x=has_baking, y=is_female)) + 
  xlab("has baking") + ylab("is female?")
p1 +
  geom_jitter(width=0.5, height=0.5)

model1 <- glm(is_female ~ height + (has_heels + has_baking), data=profiles, family=binomial())

#summary(model1)

b1 <- coefficients(model1)
#b1
#1/(1+exp(-(b1[1] + 0*b1[2])))
#1/(1+exp(-(b1[1] + 1*b1[2])))

p2 <- ggplot(data=profiles, aes(x=height, y=is_female)) +
  xlab("height") + ylab("is female") +
  xlim(c(50, 80)) + 
  geom_jitter(height=0.2) 
p2



#p1 + 
#  geom_hline(yintercept=1/(1+exp(-(b1[1] + 0*b1[2]))), col="red", size=2)+
#  geom_hline(yintercept=1/(1+exp(-(b1[1] + 1*b1[2]))), col="blue", size=2)

```
> The three factors above go into the model, which gives the fitted probabilities graphed below

### b)

Plot a histogram of the fitted probabilities $\widehat{p}_i$ for all users $i=1,
\ldots, n=59946$ in your dataset.

```{r, echo=FALSE, fig.width=12, fig.height=6}
hist(fitted(model1))

#fitted(model1)

```
> More observations fall on the ends, and there isn't much in the .4-.6 range. This is good. We don't want things to be in the middle because this means equally likely to be male or female.


### c)

Use a *decision threshold* of $p^*=0.5$ to make an explicit prediction for each
user $i$'s sex and save this in a variable `predicted_sex`. In other words, for user $i$

* If $\widehat{p}_i > p^*$, set `predicted_sex = 1` i.e. they are female
* If $\widehat{p}_i < p^*$, set `predicted_sex = 0` i.e. they are male

Display a 2 x 2 contigency table of `sex` and `predicted_sex` i.e. compare the 
predicted sex to the actual sex of all users. The sum of all the elements in
your table should be $n=59946$. Comment on how well our predictions fared.

```{r, echo=FALSE, fig.width=12, fig.height=6}
predict = c( sum(model1$fitted.values >0.5), sum(model1$fitted.values <0.5))

actual = c(sum(profiles$is_female==1), sum(profiles$is_female==0))

contig_table <- rbind(predict, actual)
contig_table

```
> It seems like it gets equal amounts almost. I couldn't merge the fitted vales back into the orgional profiles though, so I don't have a real sense of false positives or false negatives.

### d)

Say we wanted to have a **false positive rate** of about 20%, i.e. of the people
we predicted to be female, we want to be wrong no more than 20% of the time. What
decision threshold $p^*$ should we use?

```{r, echo=FALSE, fig.width=12, fig.height=6}

```





## Question 2:

Using the jukebox data, plot a time series of the number of songs played each
week over the entire time period. i.e.

* On the x-axis present actual dates (not something like Week 93, which doesn't 
mean anything to most people).
* On the y-axis present the total number of songs.

What seasonal (i.e. cyclical) patterns do you observe?

```{r, echo=FALSE, fig.width=12, fig.height=6}
attach(jukebox)

jukebox <- mutate(jukebox, parsed_date_time = parse_date_time(date_time, "%b %d %H%M%S %y"))

jukebox <- separate(jukebox, col=parsed_date_time, into=c("date", "time"), sep = " ")

jukebox$date <- as.Date(jukebox$date, "%Y-%m-%d")

#jukebox$Month <- as.Date(cut(jukebox$date, breaks = "month"))
jukebox$Week <- as.Date(cut(jukebox$date, breaks = "week"))

q2 <- jukebox %>% group_by(Week) %>% summarize(n = n())
#q2

p2 <- ggplot(q2, aes(x = Week, y = n)) + geom_point(stat="identity")
p2
```

> as one would expect, the summers and winter break there's way fewer plays.



## Question 3:

Using the jukebox data, what are the top 10 artists played during the "graveyard
shift" during the academic year? Define

* the "graveyard shift" as midnight to 8am
* the academic year as September through May (inclusive)

```{r, echo=FALSE, fig.width=12, fig.height=6}

jukebox <- separate(jukebox, col=time, into=c("Hour", "Minute", "Second"), sep = ":") %>% 
  separate(col=date_time, into=c("day", "Month_name", "junk"), sep = " ")

jukebox <- mutate(jukebox, Hour = as.numeric(Hour))

q3 <- jukebox %>% filter(Hour < 8 ) %>% filter(Month_name == "Sep" || Month_name == "Oct" ||Month_name == "Nov" ||Month_name == "Dec" ||Month_name == "Jan" ||Month_name == "Feb" ||Month_name == "Mar" ||Month_name == "Apr" ||Month_name == "May")
# there must be a fancier way to do this.

q3 <- group_by(q3, artist) %>% summarize(count = n())


q3 <- top_n(q3, 10, count)

q3$artist <- factor(q3$artist, levels = q3$artist[order(-q3$count)])
#Thanks Ali for finally showing me how to order my bar charts!

p3 <- ggplot(q3, aes(x = artist, y = count)) + geom_bar(stat="identity")
p3 <- p3 + theme(axis.text.x = element_text(angle = 45, hjust = 1))
p3

```

> It doesn't change much when you don't limit it to the school year. This is probably because the greater number of plays durring the school year drive what makes the top anyway.



## Question 4:

We want to compare the volatility of 

* bitcoin prices
* gold prices

Let our measure of volatility be the relative change from day-to-day in price. 
Let the reference currency be US dollars. Analyze these results and provide
insight to a foreign currency exchanger.

```{r, echo=FALSE, fig.width=12, fig.height=6}

```





## Question 5:

Using the data loaded from Quandl below, plot a time series using `geom_line()`
comparing cheese and milk production in the US from 1930 to today. Comment on this.

* Cheese [page](https://www.quandl.com/data/USDANASS/NASS_CHEESEPRODUCTIONMEASUREDINLB-Cheese-Production-Measured-In-Lb)
* Milk [page](https://www.quandl.com/data/USDANASS/NASS_MILKPRODUCTIONMEASUREDINLB-Milk-Production-Measured-In-Lb)

```{r, echo=FALSE, fig.width=12, fig.height=6}
cheese <- Quandl("USDANASS/NASS_CHEESEPRODUCTIONMEASUREDINLB") %>% 
  tbl_df()
milk <-  Quandl("USDANASS/NASS_MILKPRODUCTIONMEASUREDINLB") %>% 
  tbl_df()
```

